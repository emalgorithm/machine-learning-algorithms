{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from data_importing import preprocess_data, text_to_features\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, _, words_vocabulary = preprocess_data()\n",
    "X = text_to_features(X, words_vocabulary)\n",
    "            \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, n_features, n_epochs=100):\n",
    "        # Account for bias\n",
    "        self.w = np.zeros(n_features + 1)\n",
    "        self.lr = 0.01\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "    def logistic_function(self, x):\n",
    "        return 1 / (1 + np.e ** (-x))\n",
    "        \n",
    "    def prob(self, x):\n",
    "        \"\"\"Probability that x belongs to the first class\"\"\"\n",
    "        return self.logistic_function(np.dot(self.w, x))\n",
    "    \n",
    "    def add_bias(self, X):\n",
    "        return np.append(X, np.ones((len(X), 1)), axis=1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X_with_biases = self.add_bias(X)\n",
    "        for k in range(self.n_epochs):\n",
    "            gradient = np.zeros(len(self.w))\n",
    "            total_log_loss = 0\n",
    "            print(\"weights: {}\".format(self.w))\n",
    "\n",
    "            # Compute gradient\n",
    "            for i, x in enumerate(X_with_biases):\n",
    "                p = self.prob(x)\n",
    "                for j in range(len(gradient)):\n",
    "                    gradient[j] += (y[i] - p) * x[j]\n",
    "                log_loss = -(p if y[i] else 1 - p) \n",
    "                total_log_loss += log_loss\n",
    "            \n",
    "            # Don't divide gradient by number of examples otherwise step becomes too small\n",
    "            # Batch Gradient Descent\n",
    "            self.w += self.lr * gradient\n",
    "                \n",
    "            avg_log_loss = total_log_loss / len(X)\n",
    "                \n",
    "            print(\"Epoch {}, avg log loss: {}\".format(k, avg_log_loss))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return [1 if self.prob(x) >= 0.5 else 0 for x in self.add_bias(X)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [0. 0. 0. ... 0. 0. 0.]\n",
      "Epoch 0, avg log loss: -0.5\n",
      "weights: [-0.005 -0.015 -0.02  ...  0.    -0.005 -0.07 ]\n",
      "Epoch 1, avg log loss: -0.5211786459545394\n",
      "weights: [-0.00551311 -0.02803333 -0.03729169 ...  0.         -0.00986253\n",
      "  0.13802717]\n",
      "Epoch 2, avg log loss: -0.5360756857390196\n",
      "weights: [-0.01587318 -0.04603247 -0.06150241 ...  0.         -0.01542143\n",
      " -0.54863224]\n",
      "Epoch 3, avg log loss: -0.5469680987297589\n",
      "weights: [ 0.00432945 -0.04869893 -0.06467028 ...  0.         -0.0185988\n",
      "  1.31622036]\n",
      "Epoch 4, avg log loss: -0.507234920687012\n",
      "weights: [-0.04028587 -0.08483825 -0.11148971 ...  0.         -0.02714125\n",
      " -2.24830681]\n",
      "Epoch 5, avg log loss: -0.5152570214163604\n",
      "weights: [-9.99397177e-04 -7.58375209e-02 -1.01891402e-01 ...  0.00000000e+00\n",
      " -2.75529762e-02  1.53042637e+00]\n",
      "Epoch 6, avg log loss: -0.5080010331850897\n",
      "weights: [-0.04689551 -0.11308418 -0.14948084 ...  0.         -0.03645135\n",
      " -2.17180332]\n",
      "Epoch 7, avg log loss: -0.5216586514940353\n",
      "weights: [-0.00774157 -0.10429097 -0.14003876 ...  0.         -0.03691555\n",
      "  1.54285512]\n",
      "Epoch 8, avg log loss: -0.5129183171309146\n",
      "weights: [-0.05322365 -0.14155595 -0.1873164  ...  0.         -0.04581275\n",
      " -2.14648475]\n",
      "Epoch 9, avg log loss: -0.5291770307302667\n",
      "weights: [-0.01418574 -0.13288876 -0.17808072 ...  0.         -0.04628172\n",
      "  1.5006105 ]\n",
      "Epoch 10, avg log loss: -0.5197798745676813\n",
      "weights: [-0.05879851 -0.1699088  -0.22471687 ...  0.         -0.05505244\n",
      " -2.13878815]\n",
      "Epoch 11, avg log loss: -0.537335323290965\n",
      "weights: [-0.01990655 -0.1613179  -0.21578288 ...  0.         -0.05549617\n",
      "  1.43730906]\n",
      "Epoch 12, avg log loss: -0.5282059867913943\n",
      "weights: [-0.0633841  -0.19795806 -0.26154992 ...  0.         -0.06404668\n",
      " -2.13262434]\n",
      "Epoch 13, avg log loss: -0.5463984344273735\n",
      "weights: [-0.02470106 -0.18941899 -0.2530567  ...  0.         -0.06445155\n",
      "  1.36517423]\n",
      "Epoch 14, avg log loss: -0.5379593607839914\n",
      "weights: [-0.06685582 -0.22558234 -0.29776609 ...  0.         -0.07269189\n",
      " -2.12191099]\n",
      "Epoch 15, avg log loss: -0.5566925826908031\n",
      "weights: [-0.0284732  -0.21708685 -0.28988126 ...  0.         -0.07305442\n",
      "  1.28707801]\n",
      "Epoch 16, avg log loss: -0.5490110622482546\n",
      "weights: [-0.06913386 -0.25267134 -0.33334731 ...  0.         -0.08088758\n",
      " -2.1045636 ]\n",
      "Epoch 17, avg log loss: -0.5682896068449451\n",
      "weights: [-0.03117772 -0.24422377 -0.32623473 ...  0.         -0.0812097\n",
      "  1.20423919]\n",
      "Epoch 18, avg log loss: -0.5613383887588247\n",
      "weights: [-0.07018072 -0.2791128  -0.36827331 ...  0.         -0.08853461\n",
      " -2.07944268]\n",
      "Epoch 19, avg log loss: -0.5810813667442691\n",
      "weights: [-0.03282411 -0.27073048 -0.36205507 ...  0.         -0.08882101\n",
      "  1.11827708]\n",
      "Epoch 20, avg log loss: -0.5747971775156681\n",
      "weights: [-0.07002927 -0.30479804 -0.40250154 ...  0.         -0.09554463\n",
      " -2.04615664]\n",
      "Epoch 21, avg log loss: -0.5948578444263776\n",
      "weights: [-0.03349937 -0.29651076 -0.39722876 ...  0.         -0.09580101\n",
      "  1.03085166]\n",
      "Epoch 22, avg log loss: -0.5891554384865139\n",
      "weights: [-0.06879759 -0.3296271  -0.43595161 ...  0.         -0.10185374\n",
      " -2.00505423]\n",
      "Epoch 23, avg log loss: -0.6093703022437605\n",
      "weights: [-0.03336514 -0.32147541 -0.43159912 ...  0.         -0.10208578\n",
      "  0.94325266]\n",
      "Epoch 24, avg log loss: -0.6041690714313349\n",
      "weights: [-0.06667206 -0.35351053 -0.46850036 ...  0.         -0.10743421\n",
      " -1.95686987]\n",
      "Epoch 25, avg log loss: -0.6243980655636937\n",
      "weights: [-0.03261541 -0.34554376 -0.46498676 ...  0.         -0.10764717\n",
      "  0.85621785]\n",
      "Epoch 26, avg log loss: -0.6196455877156675\n",
      "weights: [-0.0638562  -0.37636938 -0.4999893  ...  0.         -0.1122986\n",
      " -1.90229213]\n",
      "Epoch 27, avg log loss: -0.6397850949793344\n",
      "weights: [-0.03140917 -0.36864418 -0.49720779 ...  0.         -0.11249722\n",
      "  0.77001176]\n",
      "Epoch 28, avg log loss: -0.635461081111431\n",
      "weights: [-0.06050566 -0.39813646 -0.53023864 ...  0.         -0.11649417\n",
      " -1.84174371]\n",
      "Epoch 29, avg log loss: -0.6554336163863713\n",
      "weights: [-0.02981812 -0.39071608 -0.52808219 ...  0.         -0.11668277\n",
      "  0.68463186]\n",
      "Epoch 30, avg log loss: -0.6515363045472107\n",
      "weights: [-0.0566861  -0.41876102 -0.55906118 ...  0.         -0.12009115\n",
      " -1.77542804]\n",
      "Epoch 31, avg log loss: -0.6712715767254305\n",
      "weights: [-0.02782146 -0.41171492 -0.55743586 ...  0.         -0.12027369\n",
      "  0.5999908 ]\n",
      "Epoch 32, avg log loss: -0.6678004031182475\n",
      "weights: [-0.05237841 -0.43821577 -0.58627473 ...  0.         -0.12316992\n",
      " -1.70350586]\n",
      "Epoch 33, avg log loss: -0.6872170348411812\n",
      "weights: [-0.0253424  -0.43161894 -0.58510384 ...  0.         -0.12335005\n",
      "  0.5160295 ]\n",
      "Epoch 34, avg log loss: -0.6841608108195437\n",
      "weights: [-0.04752145 -0.4565036  -0.61171559 ...  0.         -0.12581077\n",
      " -1.62627653]\n",
      "Epoch 35, avg log loss: -0.7031531183728675\n",
      "weights: [-0.0222968  -0.45043515 -0.61093926 ...  0.         -0.12599191\n",
      "  0.43276993]\n",
      "Epoch 36, avg log loss: -0.7004862712696064\n",
      "weights: [-0.04206133 -0.47366124 -0.63525408 ...  0.         -0.12808771\n",
      " -1.54429916]\n",
      "Epoch 37, avg log loss: -0.7189184083777608\n",
      "weights: [-0.01862922 -0.46820161 -0.6348275  ...  0.         -0.12827306\n",
      "  0.35032632]\n",
      "Epoch 38, avg log loss: -0.7166042046723013\n",
      "weights: [-0.03598302 -0.48975763 -0.65681011 ...  0.         -0.13006558\n",
      " -1.45842793]\n",
      "Epoch 39, avg log loss: -0.7343118440799613\n",
      "weights: [-0.01432841 -0.48498447 -0.65670174 ...  0.         -0.13025813\n",
      "  0.26889909]\n",
      "Epoch 40, avg log loss: -0.732309146093833\n",
      "weights: [-0.02931869 -0.50488679 -0.676365   ...  0.         -0.13179946\n",
      " -1.36977104]\n",
      "Epoch 41, avg log loss: -0.7491068867071212\n",
      "weights: [-0.00942603 -0.50086995 -0.67655508 ...  0.         -0.13200201\n",
      "  0.18877912]\n",
      "Epoch 42, avg log loss: -0.7473761967168973\n",
      "weights: [-0.02213969 -0.51915691 -0.69396665 ...  0.         -0.13333535\n",
      " -1.27961351]\n",
      "Epoch 43, avg log loss: -0.7630687601292482\n",
      "weights: [-0.00398629 -0.51595329 -0.69444568 ...  0.         -0.1335505\n",
      "  0.11037687]\n",
      "Epoch 44, avg log loss: -0.7615752004000094\n",
      "weights: [-0.0145425  -0.53267867 -0.70972698 ...  0.         -0.13471143\n",
      " -1.18934631]\n",
      "Epoch 45, avg log loss: -0.7759719733299429\n",
      "weights: [ 0.00190608 -0.53032769 -0.71049387 ...  0.         -0.1349415\n",
      "  0.03426633]\n",
      "Epoch 46, avg log loss: -0.7746859474430314\n",
      "weights: [-0.00663641 -0.54555571 -0.72381256 ...  0.         -0.1359594\n",
      " -1.10041882]\n",
      "Epoch 47, avg log loss: -0.7876194135634458\n",
      "weights: [ 0.00815543 -0.54407569 -0.72487208 ...  0.         -0.13620641\n",
      " -0.03878578]\n",
      "Epoch 48, avg log loss: -0.7865172591266539\n",
      "weights: [ 0.00146455 -0.55787876 -0.73642985 ...  0.         -0.1371057\n",
      " -1.01430112]\n",
      "Epoch 49, avg log loss: -0.7978646770679115\n",
      "weights: [ 0.01466167 -0.55726468 -0.73778931 ...  0.         -0.13737125\n",
      " -0.10783653]\n",
      "Epoch 50, avg log loss: -0.7969306202861176\n",
      "weights: [ 0.00964641 -0.56972369 -0.74780718 ...  0.         -0.13817249\n",
      " -0.93243021]\n",
      "Epoch 51, avg log loss: -0.8066352324237424\n",
      "weights: [ 0.02132513 -0.5699466  -0.74947255 ...  0.         -0.13845767\n",
      " -0.17184457]\n",
      "Epoch 52, avg log loss: -0.805863631679246\n",
      "weights: [ 0.01779947 -0.58115258 -0.75817587 ...  0.         -0.13917833\n",
      " -0.85612523]\n",
      "Epoch 53, avg log loss: -0.8139488562767547\n",
      "weights: [ 0.02805132 -0.58216087 -0.76014783 ...  0.         -0.1394836\n",
      " -0.22979929]\n",
      "Epoch 54, avg log loss: -0.8133442869903574\n",
      "weights: [ 0.02582333 -0.59221646 -0.76775327 ...  0.         -0.14013861\n",
      " -0.7864814 ]\n",
      "Epoch 55, avg log loss: -0.819914535891744\n",
      "weights: [ 0.0347557  -0.59393918 -0.77002402 ...  0.         -0.14046379\n",
      " -0.28086806]\n",
      "Epoch 56, avg log loss: -0.8194881509115638\n",
      "weights: [ 0.03363249 -0.60295832 -0.77672977 ...  0.         -0.14106588\n",
      " -0.72427201]\n",
      "Epoch 57, avg log loss: -0.824714256363101\n",
      "weights: [ 0.04136764 -0.60531016 -0.77928134 ...  0.         -0.14141011\n",
      " -0.32451925]\n",
      "Epoch 58, avg log loss: -0.8244773352502658\n",
      "weights: [ 0.04116073 -0.61341544 -0.78526148 ...  0.         -0.14197007\n",
      " -0.66988909]\n",
      "Epoch 59, avg log loss: -0.8285707450442024\n",
      "weights: [ 0.04783245 -0.61630269 -0.78806596 ...  0.         -0.14233192\n",
      " -0.36058961]\n",
      "Epoch 60, avg log loss: -0.8285281846506339\n",
      "weights: [ 0.04836305 -0.62362058 -0.7934684  ...  0.         -0.14285884\n",
      " -0.62333691]\n",
      "Epoch 61, avg log loss: -0.8317120005371429\n",
      "weights: [ 0.05411105 -0.62694729 -0.79648994 ...  0.         -0.14323642\n",
      " -0.3892872 ]\n",
      "Epoch 62, avg log loss: -0.8318583191011625\n",
      "weights: [ 0.0552147  -0.63360248 -0.8014371  ...  0.         -0.14373782\n",
      " -0.58427371]\n",
      "Epoch 63, avg log loss: -0.8343428370095737\n",
      "weights: [ 0.0601778  -0.63727593 -0.80463509 ...  0.         -0.14412895\n",
      " -0.41114084]\n",
      "Epoch 64, avg log loss: -0.8346616713318467\n",
      "weights: [ 0.06170788 -0.64338591 -0.80922617 ...  0.         -0.14461099\n",
      " -0.55208603]\n",
      "Epoch 65, avg log loss: -0.8366286249214339\n",
      "weights: [ 0.0660175  -0.64732101 -0.81255877 ...  0.         -0.1450134\n",
      " -0.42691656]\n",
      "Epoch 66, avg log loss: -0.8370951107574\n",
      "weights: [ 0.06784746 -0.65299183 -0.8168727  ...  0.         -0.14548103\n",
      " -0.52597791]\n",
      "Epoch 67, avg log loss: -0.8386909856044595\n",
      "weights: [ 0.07162235 -0.65711412 -0.82030005 ...  0.         -0.1458925\n",
      " -0.43752153]\n",
      "Epoch 68, avg log loss: -0.8392756313475113\n",
      "weights: [ 0.07364661 -0.6624378  -0.82439857 ...  0.         -0.14634963\n",
      " -0.50506069]\n",
      "Epoch 69, avg log loss: -0.8406119685123633\n",
      "weights: [ 0.07698952 -0.66668503 -0.82788516 ...  0.         -0.14676813\n",
      " -0.44391176]\n",
      "Epoch 70, avg log loss: -0.8412846420142489\n",
      "weights: [ 0.07912312 -0.67173853 -0.83181571 ...  0.         -0.14721777\n",
      " -0.4884328 ]\n",
      "Epoch 71, avg log loss: -0.8424424826262363\n",
      "weights: [ 0.08211951 -0.67606098 -0.83533174 ...  0.         -0.14764152\n",
      " -0.44701438]\n",
      "Epoch 72, avg log loss: -0.8431755679962901\n",
      "weights: [ 0.08429666 -0.68090652 -0.8391301  ...  0.         -0.14808594\n",
      " -0.47524319]\n",
      "Epoch 73, avg log loss: -0.8442115567503641\n",
      "weights: [ 0.08701504 -0.68526631 -0.84265188 ...  0.         -0.14851346\n",
      " -0.44767053]\n",
      "Epoch 74, avg log loss: -0.8449818728379672\n",
      "weights: [ 0.08918693 -0.68995267 -0.84634448 ...  0.         -0.14895431\n",
      " -0.46473507]\n",
      "Epoch 75, avg log loss: -0.8459343570560525\n",
      "weights: [ 0.09168047 -0.69432236 -0.84985414 ...  0.         -0.14938443\n",
      " -0.44660035]\n",
      "Epoch 76, avg log loss: -0.8467238674908883\n",
      "weights: [ 0.09381258 -0.69888671 -0.85346009 ...  0.         -0.14982288\n",
      " -0.45627003]\n",
      "Epoch 77, avg log loss: -0.8476181322107291\n",
      "weights: [ 0.09612142 -0.70324753 -0.85694481 ...  0.         -0.15025467\n",
      " -0.4443886 ]\n",
      "Epoch 78, avg log loss: -0.8484137386670959\n",
      "weights: [ 0.09819072 -0.70771749 -0.86047761 ...  0.         -0.15069148\n",
      " -0.44933464]\n",
      "Epoch 79, avg log loss: -0.8492660992494264\n",
      "weights: [ 0.10034448 -0.7120575  -0.86392869 ...  0.         -0.15112425\n",
      " -0.44148679]\n",
      "Epoch 80, avg log loss: -0.8500588954610789\n",
      "weights: [ 0.10233678 -0.71645321 -0.86739772 ...  0.         -0.15155992\n",
      " -0.44353386]\n",
      "Epoch 81, avg log loss: -0.850879700130714\n",
      "weights: [ 0.10435702 -0.7207655  -0.87080958 ...  0.         -0.15199315\n",
      " -0.43822672]\n",
      "Epoch 82, avg log loss: -0.8516640229297597\n",
      "weights: [ 0.10626455 -0.72510143 -0.87422123 ...  0.         -0.15242794\n",
      " -0.43857581]\n",
      "Epoch 83, avg log loss: -0.8524597573469057\n",
      "weights: [ 0.10816693 -0.72938264 -0.87759056 ...  0.         -0.15286125\n",
      " -0.43484019]\n",
      "Epoch 84, avg log loss: -0.8532322678053954\n",
      "weights: [ 0.1099864  -0.73366912 -0.88094918 ...  0.         -0.15329529\n",
      " -0.43425257]\n",
      "Epoch 85, avg log loss: -0.8540069796447652\n",
      "weights: [ 0.1117824  -0.73791821 -0.88427425 ...  0.         -0.15372838\n",
      " -0.43148068]\n",
      "Epoch 86, avg log loss: -0.8547658972627756\n",
      "weights: [ 0.11351351 -0.74216263 -0.8875828  ...  0.         -0.1541617\n",
      " -0.43042048]\n",
      "Epoch 87, avg log loss: -0.8555221325522642\n",
      "weights: [ 0.11521177 -0.74638    -0.89086288 ...  0.         -0.15459434\n",
      " -0.42824365]\n",
      "Epoch 88, avg log loss: -0.8562666600740882\n",
      "weights: [ 0.11685605 -0.7505877  -0.89412344 ...  0.         -0.15502691\n",
      " -0.42698253]\n",
      "Epoch 89, avg log loss: -0.8570060617352813\n",
      "weights: [ 0.11846331 -0.75477451 -0.89735848 ...  0.         -0.15545892\n",
      " -0.4251839 ]\n",
      "Epoch 90, avg log loss: -0.8577359867620139\n",
      "weights: [ 0.12002338 -0.75894946 -0.90057258 ...  0.         -0.15589066\n",
      " -0.42387384]\n",
      "Epoch 91, avg log loss: -0.8584596664230975\n",
      "weights: [ 0.12154517 -0.76310722 -0.90376289 ...  0.         -0.15632188\n",
      " -0.42232913]\n",
      "Epoch 92, avg log loss: -0.8591751035469319\n",
      "weights: [ 0.1230242  -0.76725243 -0.90693174 ...  0.         -0.1567527\n",
      " -0.42105073]\n",
      "Epoch 93, avg log loss: -0.8598838656176213\n",
      "weights: [ 0.12446523 -0.77138274 -0.91007787 ...  0.         -0.15718299\n",
      " -0.41968991]\n",
      "Epoch 94, avg log loss: -0.8605850991969279\n",
      "weights: [ 0.12586663 -0.77550056 -0.91320248 ...  0.         -0.1576128\n",
      " -0.41848287]\n",
      "Epoch 95, avg log loss: -0.8612795717031836\n",
      "weights: [ 0.12723109 -0.77960498 -0.91630509 ...  0.         -0.15804204\n",
      " -0.41726653]\n",
      "Epoch 96, avg log loss: -0.8619669651847931\n",
      "weights: [ 0.12855833 -0.78369728 -0.91938635 ...  0.         -0.15847073\n",
      " -0.41614815]\n",
      "Epoch 97, avg log loss: -0.8626476737859292\n",
      "weights: [ 0.12985002 -0.78777721 -0.92244617 ...  0.         -0.15889882\n",
      " -0.41505353]\n",
      "Epoch 98, avg log loss: -0.8633216202082948\n",
      "weights: [ 0.13110652 -0.79184551 -0.92548494 ...  0.         -0.15932629\n",
      " -0.41402932]\n",
      "Epoch 99, avg log loss: -0.8639890288679386\n",
      "weights: [ 0.13232899 -0.79590222 -0.92850273 ...  0.         -0.15975311\n",
      " -0.41304245]\n",
      "Epoch 100, avg log loss: -0.8646499254011413\n",
      "weights: [ 0.13351803 -0.79994779 -0.93149981 ...  0.         -0.16017926\n",
      " -0.41211206]\n",
      "Epoch 101, avg log loss: -0.865304458232129\n",
      "weights: [ 0.13467459 -0.80398235 -0.93447632 ...  0.         -0.16060473\n",
      " -0.41122347]\n",
      "Epoch 102, avg log loss: -0.8659526940062089\n",
      "weights: [ 0.13579935 -0.80800622 -0.93743249 ...  0.         -0.16102948\n",
      " -0.41038382]\n",
      "Epoch 103, avg log loss: -0.866594746887274\n",
      "weights: [ 0.13689313 -0.81201957 -0.94036849 ...  0.         -0.1614535\n",
      " -0.40958637]\n",
      "Epoch 104, avg log loss: -0.8672306977853892\n",
      "weights: [ 0.13795663 -0.81602262 -0.94328453 ...  0.         -0.16187677\n",
      " -0.40883322]\n",
      "Epoch 105, avg log loss: -0.867860644616656\n",
      "weights: [ 0.1389906  -0.82001553 -0.94618079 ...  0.         -0.16229926\n",
      " -0.40812097]\n",
      "Epoch 106, avg log loss: -0.8684846715194752\n",
      "weights: [ 0.13999571 -0.82399848 -0.94905745 ...  0.         -0.16272097\n",
      " -0.40744972]\n",
      "Epoch 107, avg log loss: -0.8691028677511968\n",
      "weights: [ 0.14097267 -0.82797161 -0.95191471 ...  0.         -0.16314186\n",
      " -0.40681744]\n",
      "Epoch 108, avg log loss: -0.8697153163826483\n",
      "weights: [ 0.14192213 -0.83193506 -0.95475275 ...  0.         -0.16356193\n",
      " -0.40622345]\n",
      "Epoch 109, avg log loss: -0.8703221011828035\n",
      "weights: [ 0.14284476 -0.83588896 -0.95757175 ...  0.         -0.16398116\n",
      " -0.40566634]\n",
      "Epoch 110, avg log loss: -0.8709233026337609\n",
      "weights: [ 0.14374116 -0.83983341 -0.96037191 ...  0.         -0.16439954\n",
      " -0.40514515]\n",
      "Epoch 111, avg log loss: -0.8715190003747998\n",
      "weights: [ 0.14461198 -0.84376851 -0.9631534  ...  0.         -0.16481704\n",
      " -0.40465872]\n",
      "Epoch 112, avg log loss: -0.8721092718688997\n",
      "weights: [ 0.1454578  -0.84769436 -0.9659164  ...  0.         -0.16523366\n",
      " -0.40420606]\n",
      "Epoch 113, avg log loss: -0.8726941932613361\n",
      "weights: [ 0.14627922 -0.85161104 -0.9686611  ...  0.         -0.16564938\n",
      " -0.40378613]\n",
      "Epoch 114, avg log loss: -0.8732738389702998\n",
      "weights: [ 0.1470768  -0.85551862 -0.97138767 ...  0.         -0.1660642\n",
      " -0.40339795]\n",
      "Epoch 115, avg log loss: -0.8738482819976885\n",
      "weights: [ 0.1478511  -0.85941717 -0.97409629 ...  0.         -0.16647809\n",
      " -0.40304056]\n",
      "Epoch 116, avg log loss: -0.8744175938283344\n",
      "weights: [ 0.14860267 -0.86330674 -0.97678714 ...  0.         -0.16689105\n",
      " -0.40271304]\n",
      "Epoch 117, avg log loss: -0.8749818445565528\n",
      "weights: [ 0.14933202 -0.8671874  -0.97946039 ...  0.         -0.16730307\n",
      " -0.40241447]\n",
      "Epoch 118, avg log loss: -0.8755411028829535\n",
      "weights: [ 0.15003969 -0.87105918 -0.98211621 ...  0.         -0.16771413\n",
      " -0.402144  ]\n",
      "Epoch 119, avg log loss: -0.8760954361796416\n",
      "weights: [ 0.15072616 -0.87492212 -0.98475478 ...  0.         -0.16812423\n",
      " -0.40190077]\n",
      "Epoch 120, avg log loss: -0.8766449105151685\n",
      "weights: [ 0.15139194 -0.87877627 -0.98737627 ...  0.         -0.16853337\n",
      " -0.40168396]\n",
      "Epoch 121, avg log loss: -0.8771895906985744\n",
      "weights: [ 0.15203749 -0.88262165 -0.98998084 ...  0.         -0.16894152\n",
      " -0.40149276]\n",
      "Epoch 122, avg log loss: -0.8777295403106687\n",
      "weights: [ 0.15266328 -0.8864583  -0.99256867 ...  0.         -0.16934869\n",
      " -0.40132641]\n",
      "Epoch 123, avg log loss: -0.8782648217397288\n",
      "weights: [ 0.15326978 -0.89028622 -0.99513991 ...  0.         -0.16975486\n",
      " -0.40118415]\n",
      "Epoch 124, avg log loss: -0.8787954962126924\n",
      "weights: [ 0.15385741 -0.89410546 -0.99769474 ...  0.         -0.17016004\n",
      " -0.40106524]\n",
      "Epoch 125, avg log loss: -0.8793216238266273\n",
      "weights: [ 0.15442661 -0.89791601 -1.00023332 ...  0.         -0.17056421\n",
      " -0.40096899]\n",
      "Epoch 126, avg log loss: -0.8798432635781243\n",
      "weights: [ 0.1549778  -0.9017179  -1.0027558  ...  0.         -0.17096737\n",
      " -0.4008947 ]\n",
      "Epoch 127, avg log loss: -0.8803604733919592\n",
      "weights: [ 0.15551139 -0.90551113 -1.00526236 ...  0.         -0.17136951\n",
      " -0.40084171]\n",
      "Epoch 128, avg log loss: -0.8808733101484095\n",
      "weights: [ 0.15602778 -0.90929572 -1.00775314 ...  0.         -0.17177063\n",
      " -0.40080938]\n",
      "Epoch 129, avg log loss: -0.8813818297096724\n",
      "weights: [ 0.15652736 -0.91307168 -1.01022831 ...  0.         -0.17217072\n",
      " -0.40079706]\n",
      "Epoch 130, avg log loss: -0.8818860869452066\n",
      "weights: [ 0.15701052 -0.916839   -1.01268802 ...  0.         -0.17256978\n",
      " -0.40080416]\n",
      "Epoch 131, avg log loss: -0.8823861357562041\n",
      "weights: [ 0.15747761 -0.9205977  -1.01513242 ...  0.         -0.17296781\n",
      " -0.40083008]\n",
      "Epoch 132, avg log loss: -0.8828820290991528\n",
      "weights: [ 0.15792901 -0.92434777 -1.01756167 ...  0.         -0.17336481\n",
      " -0.40087426]\n",
      "Epoch 133, avg log loss: -0.883373819008579\n",
      "weights: [ 0.15836506 -0.92808922 -1.01997592 ...  0.         -0.17376076\n",
      " -0.40093613]\n",
      "Epoch 134, avg log loss: -0.8838615566189997\n",
      "weights: [ 0.15878611 -0.93182205 -1.02237531 ...  0.         -0.17415567\n",
      " -0.40101515]\n",
      "Epoch 135, avg log loss: -0.8843452921861225\n",
      "weights: [ 0.1591925  -0.93554625 -1.02475999 ...  0.         -0.17454954\n",
      " -0.4011108 ]\n",
      "Epoch 136, avg log loss: -0.8848250751073365\n",
      "weights: [ 0.15958455 -0.93926183 -1.02713012 ...  0.         -0.17494236\n",
      " -0.40122258]\n",
      "Epoch 137, avg log loss: -0.885300953941531\n",
      "weights: [ 0.15996259 -0.94296879 -1.02948583 ...  0.         -0.17533413\n",
      " -0.40134998]\n",
      "Epoch 138, avg log loss: -0.8857729764282634\n",
      "weights: [ 0.16032692 -0.94666712 -1.03182728 ...  0.         -0.17572485\n",
      " -0.40149253]\n",
      "Epoch 139, avg log loss: -0.8862411895063259\n",
      "weights: [ 0.16067784 -0.95035682 -1.03415459 ...  0.         -0.17611452\n",
      " -0.40164976]\n",
      "Epoch 140, avg log loss: -0.8867056393317172\n",
      "weights: [ 0.16101567 -0.95403788 -1.03646791 ...  0.         -0.17650314\n",
      " -0.40182123]\n",
      "Epoch 141, avg log loss: -0.8871663712950543\n",
      "weights: [ 0.16134067 -0.95771031 -1.03876738 ...  0.         -0.17689071\n",
      " -0.40200649]\n",
      "Epoch 142, avg log loss: -0.8876234300384753\n",
      "weights: [ 0.16165315 -0.9613741  -1.04105313 ...  0.         -0.17727723\n",
      " -0.40220512]\n",
      "Epoch 143, avg log loss: -0.8880768594719923\n",
      "weights: [ 0.16195336 -0.96502925 -1.04332531 ...  0.         -0.17766269\n",
      " -0.40241671]\n",
      "Epoch 144, avg log loss: -0.8885267027894127\n",
      "weights: [ 0.16224159 -0.96867575 -1.04558404 ...  0.         -0.1780471\n",
      " -0.40264085]\n",
      "Epoch 145, avg log loss: -0.8889730024837311\n",
      "weights: [ 0.1625181  -0.9723136  -1.04782946 ...  0.         -0.17843045\n",
      " -0.40287716]\n",
      "Epoch 146, avg log loss: -0.8894158003621194\n",
      "weights: [ 0.16278314 -0.9759428  -1.0500617  ...  0.         -0.17881276\n",
      " -0.40312525]\n",
      "Epoch 147, avg log loss: -0.8898551375604902\n",
      "weights: [ 0.16303696 -0.97956334 -1.05228088 ...  0.         -0.17919401\n",
      " -0.40338477]\n",
      "Epoch 148, avg log loss: -0.8902910545576086\n",
      "weights: [ 0.1632798  -0.98317523 -1.05448714 ...  0.         -0.17957422\n",
      " -0.40365534]\n",
      "Epoch 149, avg log loss: -0.8907235911888488\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(len(X[0]), n_epochs=150)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.00% of sentences are correctly classified\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "n_correct = sum(1 for i, _ in enumerate(y_pred) if y_pred[i] == y_test[i])\n",
    "\n",
    "print(\"{0:.2f}% of sentences are correctly classified\".format(n_correct * 100 / len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.50% of sentences are correctly classified\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "n_correct = sum(1 for i, _ in enumerate(y_pred) if y_pred[i] == y_test[i])\n",
    "\n",
    "print(\"{0:.2f}% of sentences are correctly classified\".format(n_correct * 100 / len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
