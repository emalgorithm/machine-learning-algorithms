{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from data_importing import preprocess_data, text_to_features\n",
    "import scipy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, _, words_vocabulary = preprocess_data()\n",
    "X = text_to_features(X, words_vocabulary)\n",
    "            \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, n_features, n_epochs=100):\n",
    "        # Account for bias\n",
    "        self.w = np.zeros(n_features + 1)\n",
    "        self.lr = 0.005\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "    def prob(self, x):\n",
    "        \"\"\"Probability that x belongs to the first class\"\"\"\n",
    "        return scipy.special.expit(np.dot(self.w, x))\n",
    "    \n",
    "    def add_bias(self, X):\n",
    "        return np.append(X, np.ones((len(X), 1)), axis=1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X_with_biases = self.add_bias(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        for k in range(self.n_epochs):\n",
    "            gradient = np.zeros(len(self.w))\n",
    "            total_log_likelihood = 0\n",
    "            \n",
    "            ps = scipy.special.expit(np.matmul(X_with_biases, self.w))\n",
    "            gradient = sum([(y[i] - ps[i]) * x for i, x in enumerate(X_with_biases)])\n",
    "            \n",
    "            # Compute log likelihood\n",
    "            ps[y == 0] = 1 - ps[y == 0]\n",
    "            log_likelihood = sum(ps)\n",
    "            \n",
    "            # Don't divide gradient by number of examples otherwise step becomes too small\n",
    "            # Batch Gradient Descent\n",
    "            self.w += self.lr * gradient\n",
    "                \n",
    "            avg_log_likelihood = log_likelihood / len(X)\n",
    "                \n",
    "#             print(\"Epoch {}, avg log likelihood: {}\".format(k, avg_log_likelihood))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return [1 if self.prob(x) >= 0.5 else 0 for x in self.add_bias(X)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(len(X[0]), n_epochs=500)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.50% of sentences are correctly classified\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "n_correct = sum(1 for i, _ in enumerate(y_pred) if y_pred[i] == y_test[i])\n",
    "\n",
    "print(\"{0:.2f}% of sentences are correctly classified\".format(n_correct * 100 / len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.50% of sentences are correctly classified\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "n_correct = sum(1 for i, _ in enumerate(y_pred) if y_pred[i] == y_test[i])\n",
    "\n",
    "print(\"{0:.2f}% of sentences are correctly classified\".format(n_correct * 100 / len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
