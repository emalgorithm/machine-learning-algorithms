{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from data_importing import preprocess_data, text_to_features\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, _, words_vocabulary = preprocess_data()\n",
    "X = text_to_features(X, words_vocabulary)\n",
    "            \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, n_features, n_epochs=100):\n",
    "        # Account for bias\n",
    "        self.w = np.zeros(n_features + 1)\n",
    "        self.lr = 0.01\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "    def logistic_function(self, x):\n",
    "        return 1 / (1 + np.e ** (-x))\n",
    "        \n",
    "    def prob(self, x):\n",
    "        \"\"\"Probability that x belongs to the first class\"\"\"\n",
    "        return self.logistic_function(np.dot(self.w, x))\n",
    "    \n",
    "    def add_bias(self, X):\n",
    "        return np.append(X, np.ones((len(X), 1)), axis=1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X_with_biases = self.add_bias(X)\n",
    "        for k in range(self.n_epochs):\n",
    "            gradient = np.zeros(len(self.w))\n",
    "            total_log_loss = 0\n",
    "            print(\"weights: {}\".format(self.w))\n",
    "\n",
    "            for i, x in enumerate(X_with_biases):\n",
    "                p = self.prob(x)\n",
    "                for j in range(len(gradient)):\n",
    "                    gradient[j] += (y[i] - p) * x[j]\n",
    "                log_loss = -(p if y[i] else 1 - p) \n",
    "                total_log_loss += log_loss\n",
    "\n",
    "            for j in range(len(self.w)):\n",
    "                self.w[j] = self.w[j] + self.lr * gradient[j]\n",
    "                \n",
    "            avg_log_loss = total_log_loss / len(X)\n",
    "                \n",
    "            print(\"Epoch {}, avg log loss: {}\".format(k, avg_log_loss))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return [1 if self.prob(x) >= 0.5 else 0 for x in self.add_bias(X)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [0. 0. 0. ... 0. 0. 0.]\n",
      "Epoch 0, avg log loss: -0.5\n",
      "weights: [-0.005 -0.015 -0.02  ...  0.    -0.005 -0.07 ]\n",
      "Epoch 1, avg log loss: -0.5211786459545394\n",
      "weights: [-0.00551311 -0.02803333 -0.03729169 ...  0.         -0.00986253\n",
      "  0.13802717]\n",
      "Epoch 2, avg log loss: -0.5360756857390196\n",
      "weights: [-0.01587318 -0.04603247 -0.06150241 ...  0.         -0.01542143\n",
      " -0.54863224]\n",
      "Epoch 3, avg log loss: -0.5469680987297589\n",
      "weights: [ 0.00432945 -0.04869893 -0.06467028 ...  0.         -0.0185988\n",
      "  1.31622036]\n",
      "Epoch 4, avg log loss: -0.507234920687012\n",
      "weights: [-0.04028587 -0.08483825 -0.11148971 ...  0.         -0.02714125\n",
      " -2.24830681]\n",
      "Epoch 5, avg log loss: -0.5152570214163604\n",
      "weights: [-9.99397177e-04 -7.58375209e-02 -1.01891402e-01 ...  0.00000000e+00\n",
      " -2.75529762e-02  1.53042637e+00]\n",
      "Epoch 6, avg log loss: -0.5080010331850897\n",
      "weights: [-0.04689551 -0.11308418 -0.14948084 ...  0.         -0.03645135\n",
      " -2.17180332]\n",
      "Epoch 7, avg log loss: -0.5216586514940353\n",
      "weights: [-0.00774157 -0.10429097 -0.14003876 ...  0.         -0.03691555\n",
      "  1.54285512]\n",
      "Epoch 8, avg log loss: -0.5129183171309146\n",
      "weights: [-0.05322365 -0.14155595 -0.1873164  ...  0.         -0.04581275\n",
      " -2.14648475]\n",
      "Epoch 9, avg log loss: -0.5291770307302667\n",
      "weights: [-0.01418574 -0.13288876 -0.17808072 ...  0.         -0.04628172\n",
      "  1.5006105 ]\n",
      "Epoch 10, avg log loss: -0.5197798745676813\n",
      "weights: [-0.05879851 -0.1699088  -0.22471687 ...  0.         -0.05505244\n",
      " -2.13878815]\n",
      "Epoch 11, avg log loss: -0.537335323290965\n",
      "weights: [-0.01990655 -0.1613179  -0.21578288 ...  0.         -0.05549617\n",
      "  1.43730906]\n",
      "Epoch 12, avg log loss: -0.5282059867913943\n",
      "weights: [-0.0633841  -0.19795806 -0.26154992 ...  0.         -0.06404668\n",
      " -2.13262434]\n",
      "Epoch 13, avg log loss: -0.5463984344273735\n",
      "weights: [-0.02470106 -0.18941899 -0.2530567  ...  0.         -0.06445155\n",
      "  1.36517423]\n",
      "Epoch 14, avg log loss: -0.5379593607839914\n",
      "weights: [-0.06685582 -0.22558234 -0.29776609 ...  0.         -0.07269189\n",
      " -2.12191099]\n",
      "Epoch 15, avg log loss: -0.5566925826908031\n",
      "weights: [-0.0284732  -0.21708685 -0.28988126 ...  0.         -0.07305442\n",
      "  1.28707801]\n",
      "Epoch 16, avg log loss: -0.5490110622482546\n",
      "weights: [-0.06913386 -0.25267134 -0.33334731 ...  0.         -0.08088758\n",
      " -2.1045636 ]\n",
      "Epoch 17, avg log loss: -0.5682896068449451\n",
      "weights: [-0.03117772 -0.24422377 -0.32623473 ...  0.         -0.0812097\n",
      "  1.20423919]\n",
      "Epoch 18, avg log loss: -0.5613383887588247\n",
      "weights: [-0.07018072 -0.2791128  -0.36827331 ...  0.         -0.08853461\n",
      " -2.07944268]\n",
      "Epoch 19, avg log loss: -0.5810813667442691\n",
      "weights: [-0.03282411 -0.27073048 -0.36205507 ...  0.         -0.08882101\n",
      "  1.11827708]\n",
      "Epoch 20, avg log loss: -0.5747971775156681\n",
      "weights: [-0.07002927 -0.30479804 -0.40250154 ...  0.         -0.09554463\n",
      " -2.04615664]\n",
      "Epoch 21, avg log loss: -0.5948578444263776\n",
      "weights: [-0.03349937 -0.29651076 -0.39722876 ...  0.         -0.09580101\n",
      "  1.03085166]\n",
      "Epoch 22, avg log loss: -0.5891554384865139\n",
      "weights: [-0.06879759 -0.3296271  -0.43595161 ...  0.         -0.10185374\n",
      " -2.00505423]\n",
      "Epoch 23, avg log loss: -0.6093703022437605\n",
      "weights: [-0.03336514 -0.32147541 -0.43159912 ...  0.         -0.10208578\n",
      "  0.94325266]\n",
      "Epoch 24, avg log loss: -0.6041690714313349\n",
      "weights: [-0.06667206 -0.35351053 -0.46850036 ...  0.         -0.10743421\n",
      " -1.95686987]\n",
      "Epoch 25, avg log loss: -0.6243980655636937\n",
      "weights: [-0.03261541 -0.34554376 -0.46498676 ...  0.         -0.10764717\n",
      "  0.85621785]\n",
      "Epoch 26, avg log loss: -0.6196455877156675\n",
      "weights: [-0.0638562  -0.37636938 -0.4999893  ...  0.         -0.1122986\n",
      " -1.90229213]\n",
      "Epoch 27, avg log loss: -0.6397850949793344\n",
      "weights: [-0.03140917 -0.36864418 -0.49720779 ...  0.         -0.11249722\n",
      "  0.77001176]\n",
      "Epoch 28, avg log loss: -0.635461081111431\n",
      "weights: [-0.06050566 -0.39813646 -0.53023864 ...  0.         -0.11649417\n",
      " -1.84174371]\n",
      "Epoch 29, avg log loss: -0.6554336163863713\n",
      "weights: [-0.02981812 -0.39071608 -0.52808219 ...  0.         -0.11668277\n",
      "  0.68463186]\n",
      "Epoch 30, avg log loss: -0.6515363045472107\n",
      "weights: [-0.0566861  -0.41876102 -0.55906118 ...  0.         -0.12009115\n",
      " -1.77542804]\n",
      "Epoch 31, avg log loss: -0.6712715767254305\n",
      "weights: [-0.02782146 -0.41171492 -0.55743586 ...  0.         -0.12027369\n",
      "  0.5999908 ]\n",
      "Epoch 32, avg log loss: -0.6678004031182475\n",
      "weights: [-0.05237841 -0.43821577 -0.58627473 ...  0.         -0.12316992\n",
      " -1.70350586]\n",
      "Epoch 33, avg log loss: -0.6872170348411812\n",
      "weights: [-0.0253424  -0.43161894 -0.58510384 ...  0.         -0.12335005\n",
      "  0.5160295 ]\n",
      "Epoch 34, avg log loss: -0.6841608108195437\n",
      "weights: [-0.04752145 -0.4565036  -0.61171559 ...  0.         -0.12581077\n",
      " -1.62627653]\n",
      "Epoch 35, avg log loss: -0.7031531183728675\n",
      "weights: [-0.0222968  -0.45043515 -0.61093926 ...  0.         -0.12599191\n",
      "  0.43276993]\n",
      "Epoch 36, avg log loss: -0.7004862712696064\n",
      "weights: [-0.04206133 -0.47366124 -0.63525408 ...  0.         -0.12808771\n",
      " -1.54429916]\n",
      "Epoch 37, avg log loss: -0.7189184083777608\n",
      "weights: [-0.01862922 -0.46820161 -0.6348275  ...  0.         -0.12827306\n",
      "  0.35032632]\n",
      "Epoch 38, avg log loss: -0.7166042046723013\n",
      "weights: [-0.03598302 -0.48975763 -0.65681011 ...  0.         -0.13006558\n",
      " -1.45842793]\n",
      "Epoch 39, avg log loss: -0.7343118440799613\n",
      "weights: [-0.01432841 -0.48498447 -0.65670174 ...  0.         -0.13025813\n",
      "  0.26889909]\n",
      "Epoch 40, avg log loss: -0.732309146093833\n",
      "weights: [-0.02931869 -0.50488679 -0.676365   ...  0.         -0.13179946\n",
      " -1.36977104]\n",
      "Epoch 41, avg log loss: -0.7491068867071212\n",
      "weights: [-0.00942603 -0.50086995 -0.67655508 ...  0.         -0.13200201\n",
      "  0.18877912]\n",
      "Epoch 42, avg log loss: -0.7473761967168973\n",
      "weights: [-0.02213969 -0.51915691 -0.69396665 ...  0.         -0.13333535\n",
      " -1.27961351]\n",
      "Epoch 43, avg log loss: -0.7630687601292482\n",
      "weights: [-0.00398629 -0.51595329 -0.69444568 ...  0.         -0.1335505\n",
      "  0.11037687]\n",
      "Epoch 44, avg log loss: -0.7615752004000094\n",
      "weights: [-0.0145425  -0.53267867 -0.70972698 ...  0.         -0.13471143\n",
      " -1.18934631]\n",
      "Epoch 45, avg log loss: -0.7759719733299429\n",
      "weights: [ 0.00190608 -0.53032769 -0.71049387 ...  0.         -0.1349415\n",
      "  0.03426633]\n",
      "Epoch 46, avg log loss: -0.7746859474430314\n",
      "weights: [-0.00663641 -0.54555571 -0.72381256 ...  0.         -0.1359594\n",
      " -1.10041882]\n",
      "Epoch 47, avg log loss: -0.7876194135634458\n",
      "weights: [ 0.00815543 -0.54407569 -0.72487208 ...  0.         -0.13620641\n",
      " -0.03878578]\n",
      "Epoch 48, avg log loss: -0.7865172591266539\n",
      "weights: [ 0.00146455 -0.55787876 -0.73642985 ...  0.         -0.1371057\n",
      " -1.01430112]\n",
      "Epoch 49, avg log loss: -0.7978646770679115\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(len(X[0]), n_epochs=50)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.00% of sentences are correctly classified\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "n_correct = sum(1 for i, _ in enumerate(y_pred) if y_pred[i] == y_test[i])\n",
    "\n",
    "print(\"{0:.2f}% of sentences are correctly classified\".format(n_correct * 100 / len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.50% of sentences are correctly classified\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "n_correct = sum(1 for i, _ in enumerate(y_pred) if y_pred[i] == y_test[i])\n",
    "\n",
    "print(\"{0:.2f}% of sentences are correctly classified\".format(n_correct * 100 / len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
