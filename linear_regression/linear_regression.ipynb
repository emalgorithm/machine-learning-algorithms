{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_boston()\n",
    "# define the data/predictors as the pre-set feature names  \n",
    "X = pd.DataFrame(data.data, columns=data.feature_names).values\n",
    "\n",
    "# Put the target (housing value -- MEDV) in another DataFrame\n",
    "y = pd.DataFrame(data.target, columns=[\"MEDV\"]).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    def __init__(self, n_epochs=50, lr=0.000002, gradient_descent=False):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "        self.gradient_descent = gradient_descent\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self.fit_with_gradient_descent(X, y) if self.gradient_descent else self.fit_with_normal_equations(X, y)\n",
    "    \n",
    "    def fit_with_gradient_descent(self, X, y):\n",
    "        if y.ndim > 1:\n",
    "            y = np.squeeze(y)\n",
    "        X = self.add_bias(X)\n",
    "        self.w = np.zeros((len(X[0]),))\n",
    "        \n",
    "        for i in range(self.n_epochs):\n",
    "            # Batch Gradient Descent\n",
    "            gradient = np.zeros(len(self.w),)\n",
    "            loss = 0\n",
    "            \n",
    "            for j, x in enumerate(X):\n",
    "                y_pred = np.dot(self.w, x)\n",
    "                gradient += 2 * (y_pred - y[j]) * x\n",
    "                loss += (y_pred - y[j]) ** 2\n",
    "                \n",
    "            print(\"Epoch {0}: avg loss is {1:.2f}\".format(i, loss / len(y)))\n",
    "#             print(\"Gradient is {}\".format(gradient))\n",
    "#             print(\"Parameters is {}\".format(self.w))\n",
    "\n",
    "            self.w -= self.lr * gradient / len(y)\n",
    "        print(\"\\n Avg loss is: {0:.2f}\".format(loss / len(y)))\n",
    "    \n",
    "    def fit_with_normal_equations(self, X, y):\n",
    "        X = self.add_bias(X)\n",
    "        self.w = np.matmul(np.linalg.inv(np.matmul(X.T, X)), np.matmul(X.T, y))\n",
    "        \n",
    "        y_pred = np.matmul(X, self.w)\n",
    "        avg_loss = np.mean((y - y_pred) ** 2)\n",
    "        print(\"Avg loss is: {0:.2f}\".format(avg_loss))\n",
    "        \n",
    "    def add_bias(self, X):\n",
    "        return np.append(X, np.ones((len(X), 1)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(gradient_descent=True, n_epochs=10000)\n",
    "lr.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
