{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "sws = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "word_to_index = {}\n",
    "words_vocabulary = []\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "    idx = 0\n",
    "    with open('data/amazon_cells_labelled.txt', 'r') as f:\n",
    "        for l in f:\n",
    "            sentence, sentiment = l.split('\\t')\n",
    "            sentiment = int(sentiment)\n",
    "            y.append(sentiment)\n",
    "\n",
    "            words = tknzr.tokenize(sentence)\n",
    "            # Remove stopwords does not improve accuracy but makes the model faster as we have less words to process\n",
    "            words = [word for word in words if word not in sws]\n",
    "            X.append(words)\n",
    "            \n",
    "            for word in words:\n",
    "                if word not in word_to_index:\n",
    "                    word_to_index[word] = idx\n",
    "                    words_vocabulary.append(word)\n",
    "                    idx += 1\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def text_to_features(X):\n",
    "    X_feat = []\n",
    "    for x in X:\n",
    "        x_feat = [1 if word in x else 0 for word in words_vocabulary]\n",
    "        X_feat.append(x_feat)\n",
    "    \n",
    "    return X_feat\n",
    "        \n",
    "X, y = preprocess_data()\n",
    "X = text_to_features(X)\n",
    "            \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, n_features):\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.lr = 0.01\n",
    "        self.n_epochs = 50\n",
    "        \n",
    "    def prob(self, x):\n",
    "        \"\"\"Probability that x belongs to the first class\"\"\"\n",
    "        return math.e ** (np.dot(self.w, x)) / (1 + math.e ** (np.dot(self.w, x)))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for k in range(self.n_epochs):\n",
    "            gradient = np.zeros(len(self.w))\n",
    "            total_log_loss = 0\n",
    "            print(\"weights: {}\".format(self.w))\n",
    "\n",
    "            for i, x in enumerate(X):\n",
    "                p = self.prob(x)\n",
    "                for j in range(len(gradient)):\n",
    "                    gradient[j] -= (y[i] - p) * x[j]\n",
    "                log_loss = -(p if y[i] else 1 - p) \n",
    "                total_log_loss += log_loss\n",
    "\n",
    "            for j in range(len(self.w)):\n",
    "                self.w[j] = self.w[j] - self.lr * gradient[j]\n",
    "                \n",
    "            avg_log_loss = total_log_loss / len(X)\n",
    "                \n",
    "            print(\"Epoch {}, avg log loss: {}\".format(k, avg_log_loss))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return [1 if self.prob(x) >= 0.5 else 0 for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(len(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [0. 0. 0. ... 0. 0. 0.]\n",
      "Epoch 0, avg log loss: -0.5\n",
      "weights: [-0.005 -0.015 -0.02  ...  0.     0.    -0.005]\n",
      "Epoch 1, avg log loss: -0.5209261504091711\n",
      "weights: [-0.00707297 -0.02890279 -0.03833086 ...  0.          0.\n",
      " -0.0100375 ]\n",
      "Epoch 2, avg log loss: -0.5384455269170965\n",
      "weights: [-0.0107872  -0.04359115 -0.05798188 ...  0.          0.\n",
      " -0.01511979]\n",
      "Epoch 3, avg log loss: -0.5548847789840932\n",
      "weights: [-0.01082037 -0.05667262 -0.07499605 ...  0.          0.\n",
      " -0.01997521]\n",
      "Epoch 4, avg log loss: -0.5681343064887328\n",
      "weights: [-0.01403005 -0.07109049 -0.09409497 ...  0.          0.\n",
      " -0.02479077]\n",
      "Epoch 5, avg log loss: -0.5818273285073506\n",
      "weights: [-0.01243363 -0.08327349 -0.1096133  ...  0.          0.\n",
      " -0.02926547]\n",
      "Epoch 6, avg log loss: -0.5920242949027255\n",
      "weights: [-0.01553357 -0.09742364 -0.12812278 ...  0.          0.\n",
      " -0.03371675]\n",
      "Epoch 7, avg log loss: -0.6040058981611636\n",
      "weights: [-0.0126728  -0.10872383 -0.14223119 ...  0.          0.\n",
      " -0.03777629]\n",
      "Epoch 8, avg log loss: -0.611987309634728\n",
      "weights: [-0.01571692 -0.12254517 -0.16007632 ...  0.          0.\n",
      " -0.04185493]\n",
      "Epoch 9, avg log loss: -0.6227798627011804\n",
      "weights: [-0.01202302 -0.13308439 -0.17302772 ...  0.          0.\n",
      " -0.04552969]\n",
      "Epoch 10, avg log loss: -0.6292399966188299\n",
      "weights: [-0.01482386 -0.14645236 -0.19005509 ...  0.          0.\n",
      " -0.04925624]\n",
      "Epoch 11, avg log loss: -0.6390486079984138\n",
      "weights: [-0.01077773 -0.15642451 -0.20217723 ...  0.          0.\n",
      " -0.05259899]\n",
      "Epoch 12, avg log loss: -0.6445664841675932\n",
      "weights: [-0.01304966 -0.16918973 -0.21820674 ...  0.          0.\n",
      " -0.05600251]\n",
      "Epoch 13, avg log loss: -0.6534045410159662\n",
      "weights: [-0.00908595 -0.17879832 -0.22980515 ...  0.          0.\n",
      " -0.05906775]\n",
      "Epoch 14, avg log loss: -0.6583921336539188\n",
      "weights: [-0.01059211 -0.19084309 -0.24471034 ...  0.          0.\n",
      " -0.0621804 ]\n",
      "Epoch 15, avg log loss: -0.6662095333452512\n",
      "weights: [-0.00700493 -0.20024382 -0.25599827 ...  0.          0.\n",
      " -0.06501479]\n",
      "Epoch 16, avg log loss: -0.6709039226284104\n",
      "weights: [-0.00764877 -0.21152274 -0.26975754 ...  0.          0.\n",
      " -0.06787045]\n",
      "Epoch 17, avg log loss: -0.6777014537910283\n",
      "weights: [-0.00455494 -0.22079561 -0.28083358 ...  0.          0.\n",
      " -0.07050996]\n",
      "Epoch 18, avg log loss: -0.6822092434003111\n",
      "weights: [-0.00439135 -0.2313416  -0.29352894 ...  0.          0.\n",
      " -0.07314264]\n",
      "Epoch 19, avg log loss: -0.688084391555701\n",
      "weights: [-0.00175894 -0.24049693 -0.30439761 ...  0.          0.\n",
      " -0.07561373]\n",
      "Epoch 20, avg log loss: -0.6924338755323824\n",
      "weights: [-0.00094457 -0.25039771 -0.31617458 ...  0.          0.\n",
      " -0.07805549]\n",
      "Epoch 21, avg log loss: -0.6975510609025719\n",
      "weights: [ 0.00134088 -0.25940378 -0.32678918 ...  0.          0.\n",
      " -0.08037789]\n",
      "Epoch 22, avg log loss: -0.7017305264921941\n",
      "weights: [ 0.00261662 -0.26876735 -0.33780786 ...  0.          0.\n",
      " -0.08265699]\n",
      "Epoch 23, avg log loss: -0.7062633955049985\n",
      "weights: [ 0.00468559 -0.27758145 -0.34811225 ...  0.          0.\n",
      " -0.08484641]\n",
      "Epoch 24, avg log loss: -0.7102484261258908\n",
      "weights: [ 0.00625394 -0.28650813 -0.35851261 ...  0.          0.\n",
      " -0.08698641]\n",
      "Epoch 25, avg log loss: -0.7143401519646652\n",
      "weights: [ 0.00821112 -0.2950974  -0.36846682 ...  0.          0.\n",
      " -0.08905633]\n",
      "Epoch 26, avg log loss: -0.7181120748263257\n",
      "weights: [ 0.00994796 -0.30366619 -0.37835502 ...  0.          0.\n",
      " -0.09107623]\n",
      "Epoch 27, avg log loss: -0.7218646661196181\n",
      "weights: [ 0.01185915 -0.312015   -0.38794263 ...  0.          0.\n",
      " -0.09303873]\n",
      "Epoch 28, avg log loss: -0.7254182872754975\n",
      "weights: [ 0.013685   -0.32028232 -0.39739296 ...  0.          0.\n",
      " -0.09495365]\n",
      "Epoch 29, avg log loss: -0.7288992815656548\n",
      "weights: [ 0.01558258 -0.32839019 -0.40661694 ...  0.          0.\n",
      " -0.0968196 ]\n",
      "Epoch 30, avg log loss: -0.7322416267669936\n",
      "weights: [ 0.01745106 -0.33639498 -0.41568037 ...  0.          0.\n",
      " -0.0986417 ]\n",
      "Epoch 31, avg log loss: -0.7354952930328644\n",
      "weights: [ 0.01934592 -0.34427107 -0.42455533 ...  0.          0.\n",
      " -0.10042081]\n",
      "Epoch 32, avg log loss: -0.7386405568959592\n",
      "weights: [ 0.02123093 -0.35204094 -0.43326822 ...  0.          0.\n",
      " -0.10216001]\n",
      "Epoch 33, avg log loss: -0.7416972144003079\n",
      "weights: [ 0.02312289 -0.35969892 -0.44181387 ...  0.          0.\n",
      " -0.10386084]\n",
      "Epoch 34, avg log loss: -0.7446621194983749\n",
      "weights: [ 0.02500924 -0.36725482 -0.45020419 ...  0.          0.\n",
      " -0.10552544]\n",
      "Epoch 35, avg log loss: -0.7475441323634817\n",
      "weights: [ 0.02689374 -0.37470961 -0.45844133 ...  0.          0.\n",
      " -0.10715543]\n",
      "Epoch 36, avg log loss: -0.7503451557195231\n",
      "weights: [ 0.0287717  -0.38206869 -0.4665322  ...  0.          0.\n",
      " -0.1087525 ]\n",
      "Epoch 37, avg log loss: -0.7530702493766807\n",
      "weights: [ 0.03064306 -0.38933482 -0.47448084 ...  0.          0.\n",
      " -0.11031812]\n",
      "Epoch 38, avg log loss: -0.7557224421339872\n",
      "weights: [ 0.03250566 -0.39651185 -0.48229236 ...  0.          0.\n",
      " -0.11185373]\n",
      "Epoch 39, avg log loss: -0.7583053869578231\n",
      "weights: [ 0.03435853 -0.4036028  -0.489971   ...  0.          0.\n",
      " -0.11336063]\n",
      "Epoch 40, avg log loss: -0.7608220898938711\n",
      "weights: [ 0.03620034 -0.41061087 -0.4975211  ...  0.          0.\n",
      " -0.11484006]\n",
      "Epoch 41, avg log loss: -0.7632755492696497\n",
      "weights: [ 0.03803008 -0.41753893 -0.50494666 ...  0.          0.\n",
      " -0.11629315]\n",
      "Epoch 42, avg log loss: -0.7656684838894229\n",
      "weights: [ 0.03984673 -0.4243898  -0.51225155 ...  0.          0.\n",
      " -0.117721  ]\n",
      "Epoch 43, avg log loss: -0.7680034804567293\n",
      "weights: [ 0.04164942 -0.4311661  -0.51943943 ...  0.          0.\n",
      " -0.11912462]\n",
      "Epoch 44, avg log loss: -0.770282949303106\n",
      "weights: [ 0.04343733 -0.43787037 -0.52651382 ...  0.          0.\n",
      " -0.12050494]\n",
      "Epoch 45, avg log loss: -0.7725091623550533\n",
      "weights: [ 0.04520974 -0.444505   -0.53347808 ...  0.          0.\n",
      " -0.12186288]\n",
      "Epoch 46, avg log loss: -0.7746842539929512\n",
      "weights: [ 0.046966   -0.45107227 -0.5403354  ...  0.          0.\n",
      " -0.12319927]\n",
      "Epoch 47, avg log loss: -0.7768102357912697\n",
      "weights: [ 0.0487055  -0.45757435 -0.54708887 ...  0.          0.\n",
      " -0.12451489]\n",
      "Epoch 48, avg log loss: -0.7788890041830405\n",
      "weights: [ 0.05042771 -0.46401332 -0.5537414  ...  0.          0.\n",
      " -0.12581052]\n",
      "Epoch 49, avg log loss: -0.7809223496711672\n",
      "78.50% of sentences are correctly classified\n"
     ]
    }
   ],
   "source": [
    "n_correct = 0\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "n_correct = sum(1 for i, _ in enumerate(y_pred) if y_pred[i] == y_test[i])\n",
    "\n",
    "print(\"{0:.2f}% of sentences are correctly classified\".format(n_correct * 100 / len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
